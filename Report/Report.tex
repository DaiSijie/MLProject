%
% File naaclhlt2010.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{CS 475 Intro to Machine Learning: Long Short-Term Memory Project Report}

\author{Charlie Wang\\
  Johns Hopkins University\\
  3400 N. Charles Street\\
  Baltimore, MD 21218, USA\\
  {\tt TODO: fillin...@jhu.edu}
  \And
  Gilbert Maystre \\
  Johns Hopkins University\\
  3400 N. Charles Street\\
  Baltimore, MD 21218, USA\\
  {\tt TODO: fillin...@jhu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  TODO: This document contains the instructions for preparing a camera-ready
  manuscript for the proceedings of NAACL HLT 2010. The document itself conforms
  to its own specifications, and is therefore an example of what
  your manuscript should look like.  Authors are asked to conform to
  all the directions reported in this document.
\end{abstract}

\section{Introduction}

Long Short-Term Memory (LSTM) is an artificial neural network architecture, which is in turn considered as a machine learning model. First proposed by ~\cite{Hochreiter:97}, LSTMs are a subclass of recurrent neural networks, a subclass of artificial neural networks. LSTMs build upon the decades of artificial neural network research to solve new types of problems.

The artificial neural network architecture is based on the idea of biological neurons in the brain and how neurons are organized. It was discovered that neurons receive multiple inputs from other neurons, does something simple with the inputs, and outputs a signal which can be relayed to many other neurons. In the same idea, the nodes in artificial neural networks replicate the biological neurons. Artificial neural network nodes receive multiple inputs from other nodes, which are weighted. A summation is performed on the inputs, and this sum is then passed through an activation function. Many times, the activation function is a simple logistic sigmoid function. Then, the node would output the value to other nodes through weighted connections. Although each neuron is seemingly simple of-by-themselves, together, they can solve many complex problems, including the estimation of a regression surface by ~\cite{Specht:91}. The artificial neural networks were generally organized with an input layer, which consisted of many input nodes with an identity activation function. The nodes in the input layer would be connected to the nodes in the hidden layer, where the nodes in the hidden layer would in turn be connected to the nodes in the output layer. This high-level overview is the general architecture of artificial neural networks.

Much work was done on artificial neural networks, such as adding many hidden layers to form deep learning neural networks. Another work that arose in the artificial neural network world was with recurrent neural networks. Recurrent neural networks are similar to a traditional artificial neural network architecture of input layer, hidden layer, and output layer. However, the main difference is that the weights coming out of a node does not just go to other nodes in other layers -- the weights can loop back to the node itself or other nodes in that respective layer. This new idea became the recurrent neural network.

Recurrent neural networks were able to extend upon artificial neural networks, but more was wanted from what recurrent neural networks were able to perform and achieve. This led to the birth of architecture of Long Short-Term Memory (LSTM), which is built on recurrent neural networks, which is in turn built on artificial neural networks in general. Based on the work of ~\cite{Hochreiter:97}, greater complexity was added to the nodes in the hidden layer of the neural network. To start, the idea of a hidden layer node was extended into the idea of a memory block. For each memory block, it consists of the node itself, which is renamed as a cell, and the novel idea of gates. Each memory block had two gates, an input gate and an output gate. The gates were considered multiplicative, and the output of the gates are multiplied with the respective weighted values coming from and to the cell in the memory block. To clarify, the output of the input gate is multiplied with the summed input weights to the memory block. The output of the output gate is multiplied with the output value of the memory block. The goal of the gates is to control the constant error flow through discrete time steps due to the effect of the loops in the LSTM. Thus, the need for an activation function for the cell was discarded, and instead, an edge that flows back into the input of the cell was added. The value of the cell was named cell state.

Recall that the LSTM proposed by ~\cite{Hochreiter:97} is built on the idea of recurrent neural network. Thus, the general overview of the connections between nodes can be illustrated in this way: each input node output is connected/flows to each of the memory block, each gate, and each output node. The output of the memory blocks is connected to the memory block itself (the recurrent part), the output nodes, and the gates.

More research was conducted on the LSTM architecture, and ~\cite{Gers:99} discovered that there was a flaw with the cell. In fact, due to the loop from the output of the cell to the input of the cell on each time step, ~\cite{Gers:99} discovered that the loop could cause the cell state to grow indefinitely, thus causing the whole network to be skewed so much that the output of the network is of not much use. In other words, the whole network becomes broken after too many time steps. Thus, ~\cite{Gers:99} added onto the complexity of the original LSTM architecture by adding an additional gate, which they named the forget gate. The forget gate is similar to the input and output gates, but instead of targeting the input to the cell like the input gate or the output of the cell like the output gate, the forget gate targeted that recurrent loop of the cell. Similarly, as with the input gate and the output gate, they received weighted inputs from the input nodes and the memory block itself. Thus, the number of weights in the network grew and the complexity of the memory block grew, but ~\cite{Gers:99} were able to show that this improved the network results in the long term.

However, there was still a flaw present in the LSTM architecture. The LSTM architecture after the work of ~\cite{Gers:99} was ill-suited for tasks where accurate measurement of time intervals was needed. For example, the distinction is needed in some tasks to know when spikes or other abnormalities occur either 100 times steps apart, or 101 time steps apart. Thus, ~\cite{Gers:02} proposed the idea of adding peephole weighted connection from the cell to the input, forget, and output gates. Without the peephole connections, the gates would only be able to see what the output of the cell is after being multiplied by the output of the output gate. If the output gate is closed, in other words close to 0, the gates would not be receiving an accurate picture of what the cell state is giving as an output. Thus, with the peephole connection which serve as a direction connection for gates to see what the cell state is currently like, network performance was shown to increase by ~\cite{Gers:02}.

These advancements, including many other following advancements, have made the LSTM architecture applicable to many problems out in the world today. For example, Google was able to achieve breakthrough in generating image captions with the help of the LSTM architecture combined with a deep convolutional net, which is described in ~\cite{Vinyals:14}. In fact, in this specific example, natural language captions were able to be generated on images by recognizing objects in the picture, such as what was the main subject and what the background is like.

The capabilities, complexities, and potential of LSTM is why the project was chosen to be done on the LSTM architecture and on the application of a LSTM network. For the project, an application of LSTM was chosen that incorporates some of the important advancements with the architecture, such as by ~\cite{Gers:99} and by ~\cite{Gers:02}. Thus, the project is conducted on the works of ~\cite{Gers:01}, which is on the topic of learning simple context-sensitive languages (CSL).

\section{Problem}
The research project is based on replicating the solution obtained by ~\cite{Gers:01} to the problem that their research is looking to solve. Based on ~\cite{Gers:01}, the problem that the paper looks to solve is building a network that is able to accurate classify a context-free language (CFL) and a context-sensitive language (CSL). Recall that CSL includes all CFL, and CFL includes all regular language (RL). According to ~\cite{Gers:01}, it has been shown that LSTM is able to outperform recurrent neural networks when working with RL, but they are unsure if this performance gap exists also when working with CFL and CSL though they know that recognizing CFLs require a stack which the LSTM would need to be able to replicate.

~\cite{Gers:01} set up the problem for the LSTM architecture in this way: a string belonging to the language will be presented to the LSTM. The string is formatted in such a way that it has a unique start symbol $S$ and end of string symbol $T$. For example, for the CFL language $a^nb^n$, a string example that can be presented to the LSTM is $SaaabbbT$ for $n = 3$. Furthermore, the string will be presented to the LSTM in such a way that only 1 character of the string is presented at a time until all characters (the whole string) is presented.

Thus, the goal of the LSTM is to recognize what character is presented to it and predict all the next possible characters that can follow the character just presented to it. For example, with the $SaaabbbT$ example from before, when the LSTM is presented with the character $S$, the LSTM needs to be able to predict that the characters that can follow are $T$ and $a$. When the LSTM is presented with the character $a$, the LSTM needs to be able to predict that only the characters $a$ and $b$ can follow. When the LSTM is presented with the character $b$, the network needs to be able to predict that the only characters that can follow is $b$ except for the last $b$ character presented to the network, which the LSTM needs to predict that only $T$ can follow.

After the whole string is presented to the LSTM, the network, during testing, needs to make a classification decision of whether the string that it was presented with belongs within the language or not. The criteria for classifying is that if all predictions by the LSTM were correct, then the string belongs in the language. If one or more predictions by the LSTM were made on the string, then the string does not belong in the language.

When one encounters a traditional classification problem, such as whether a string belongs in the language or not, one might expect that two sets of data need to be presented to the artificial neural network: one set consisting of strings that are in the language and another set consisting of strings that are not apart of the language. In this problem, only strings that are part of the language is presented to the LSTM network. The set of strings that are not part of the language is not needed to be presented to the LSTM network to help it learn because the LSTM network is actually learning to predict characters and "fit" the language. Presenting strings that are not apart of the language could actually skew the learning process. Thus, the set of strings that are not part of the language is only needed during testing to see if the LSTM network can classify correctly.

\section{Network Architecture and Forward Pass}
The details of the network is presented with minimal detail in the paper, just enough that someone with existing LSTM knowledge can have an idea of what the architecture looks like. For example, ~\cite{Gers:01} says that for the language $a^nb^n$ in particular, there are 38 adjustable weights. The only clarification available of those 38 weights is the text that follows in parentheses: 3 peephole, 28 unit-to-unit, and 7 bias. Figuring out exactly what all these 38 weights meant, especially what exactly are connected with respect to the 28 unit-to-unit, required sitting in front of a whiteboard and attempting to draw out the network with the weights.  Thus, extensive time was placed into understanding what the network architecture looked like such that understanding of how the network operates and coding of the network could be achieved. The following is of our best understanding of the network architecture.

The LSTM architecture consists of three layers: an input layer, a hidden layer, and an output layer. In the input layer, the number of nodes corresponds to the language that is presented to the network. In fact, the number of nodes that is needed to however many different characters are in the language plus an additional node for recognizing the $S$ and $T$. For example, with the $a^nb^n$ language, there will be one node for the $a$, one node for the $b$ and one node for the $S/T$ for a total of three nodes in the input layer. This is done in such a way so that the LSTM network can recognize which character from the string is presented to the network. Similarly, since the LSTM network is predicting what the next character will be, the number of nodes in the output layer would be the same as the number of nodes in the input layer.

The hidden layer consists of memory block units. Each memory block unit has one input gate, one forget gate, and one output gate. Although it is possible for a memory block to have multiple cells, ~\cite{Gers:01} stated that their networks have only one cell per memory block. This decision was reached possibly after testing and to better see how the network is able to predict since there are less variables involved. For each language, though it is not stated by ~\cite{Gers:01}, we theorize that they chose the number of memory block units needed based on how the stack(s) would operate for recognizing the language. Thus, the number of memory block units that are needed is likely based on background knowledge of the language itself. Thus, for the language $a^nb^n$, only one memory block is in the hidden layer. However, for the language $a^nb^mB^mA^n$, two memory blocks are in the hidden layer. Additionally, peepholes exist from the input gate, forget gate, and output gate to the cell for each respective memory block.

To better understand exactly what is happening in each layer, more details will be clarified layer by layer. Starting with the input layer, the encoded vector of the character from the string is presented to the nodes in the input layer. A +1 value for the element of the vector indicates that particular character being present. A -1 value indicates that it is not that character. For example, with the language $a^nb^n$, when the $a$ character is presented, the encoded vector of [$S/T$, $a$, $b$] would be [-1, 1, -1].

The nodes in the input layer take the respective input (element from the encoded vector), run it through an activation function consisting of the identity function, and output it. The output from each input layer node runs through each of the connections coming out of the input nodes. In the LSTM architecture, each input node is connected to all output nodes, each memory block, and the input gate, forget gate, and output gate of each respective memory block. Also, since the connections are weighted in the LSTM architecture, each output from the input nodes are multiplied by the weight of the connection upon reaching the destination node.

Recall that the hidden layer consists of memory blocks. Each memory block contains 1 input gate, 1 forget gate, 1 output gate, and 1 cell. The inputs to the memory block is summed. Then, the resulting value is multiplied with the output from the input gate. The new resulting value is then passed to the cell, summed with another connection called the CEC (recurrent connection from the cell's output to the cell's input), and it is stored in the cell as the cell state. Also, take note that the CEC, before it is summed, is also multiplied with the output from the forget gate. Now, the output from the cell is multiplied with the output from the output gate, and it is now passed out to the peephole connections (connected to all three gates) and out of the memory block as the memory block's output. The memory block's output is passed via weighted connections to all the output nodes in the output layer, to all the memory block's gates (input, forget, output), and back to the memory block itself as one of the inputs to the memory block. Furthermore, take note that the activation function for the gates is the logistic sigmoid of [0,1]. Each gate also sums their respective inputs and pass it through their respective activation functions.

The output layer consists of the same number of nodes as the ones in the input layer. Similar to a node in a traditional artificial neural network, the output nodes take all the inputs, sum them, then pass them through an activation function to output a value from the node. In this case, according to the paper, the activation function for the output units is a variant of the sigmoid function: a sigmoid function with range [-2,2].

The description above serves as our understanding of the LSTM architecture and how values are passed around the layers after piecing the parts together from ~\cite{Gers:99}, ~\cite{Gers:01}, and ~\cite{Gers:02}. Also, this is based on the research that has occurred on artificial neural networks, recurrent neural networks, and LSTM in general. It has been tested to be effective to some sense, which is why we theorize it is used in such a way in this application. Take note that the architecture outlined here has elements of a traditional artificial neural network (input and output layer), recurrent neural network (how weights from the output of the hidden layer loop back to the hidden layer), and LSTM (the memory blocks, gates, and peepholes). Furthermore, this understanding was used to code the LSTM network for this project report. The code is modeled after the network architecture outlined here.

On a side note, one interesting thing to take into account is that the network architecture used in ~\cite{Gers:01} uses peepholes, which were not published until 2002 by ~\cite{Gers:02}. Thus, it is possible that while ~\cite{Gers:02} was still being written, work on ~\cite{Gers:01} was being conducted with the most recent unpublished advancements of the LSTM architecture. Thus, one can expect that some things in ~\cite{Gers:01} may be unclear. As one would expect, we had to look for many outside resources, including ~\cite{Gers:02}, to understand what was going on in the network of concern in ~\cite{Gers:01}.

\section{Backward Pass (Learning Algorithm)}
Focus on how learning occurs. Say that using supervised learning.  Is gradient learning, etc.

So have algorithm for generating the example string, then encode it in the specified format.
One of the main struggles of the research project is understanding the algorithm.
Algorithm: Talk about the algorithms that are used. Analyze the algorithm: how does it work, why it works, etc.

\section{Coding the Solution}
Coding: how approached the problem. Also talk about initialization. Also, how prepare data? first generate language. "The way the character of the example string is presented to the LSTM is in a d-dimensional array, with each element of the array corresponding to one of the input nodes assuming there are d nodes." Then get understanding of paper and how network is set up, which is highlighted in the section before under architecture. !!!Problems encountered.!!! Then coding the forward pass, derivatives, and the backward pass. How there is little information on the backward pass. Needed to go and find it. Talk about how much time spent on the project and how much work was done/

Also, analyze the algorithm using machine learning concepts/understandings.

\section{Results}
Results: analyze the results. Explain why get results.

\section{Comparison to Proposal}
Comparison to Proposal:
What was different. If different, give good explanation.

\section{Conclusion}




\section{Template: General Instructions}

Manuscripts must be in two-column format.  Exceptions to the
two-column format include the title, authors' names and complete
addresses, which must be centered at the top of the first page (see
the guidelines in Subsection~\ref{ssec:first}), and any full-width
figures or tables .  Type single-spaced.  Do not number the pages.
Start all pages directly under the top margin.  See the guidelines
later regarding formatting the first page.

%% If the paper is produced by a printer, make sure that the quality
%% of the output is dark enough to photocopy well.  It may be necessary
%% to have your laser printer adjusted for this purpose.  Papers that are too
%% faint to reproduce well may not be included.

%% {\bf Do not print page numbers on the manuscript.}  Write them lightly
%% on the back of each page in the upper left corner along with the
%% (first) author's name.

The maximum length of a manuscript is eight (8) pages for the main
conference, printed single-sided, plus one (1) page for references
(see Section~\ref{sec:length} for additional information on the
maximum number of pages).  Do not number the pages.

\subsection{Electronically-available resources}

NAACL HLT provides this description in \LaTeX2e{} ({\tt naaclhlt2010.tex}) and PDF
format ({\tt naaclhlt2010.pdf}), along with the \LaTeX2e{} style file used to
format it ({\tt naaclhlt2010.sty}) and an ACL bibliography style ({\tt naaclhlt2010.bst}).
These files are all available at
{\tt http://naaclhlt2010.isi.edu}.  A Microsoft Word
template file ({\tt naaclhlt2010.dot}) is also available at the same URL. We
strongly recommend the use of these style files, which have been
appropriately tailored for the NAACL HLT 2010 proceedings.


\subsection{Format of Electronic Manuscript}
\label{sect:pdf}

For the production of the electronic manuscript you must use Adobe's
Portable Document Format (PDF). This format can be generated from
postscript files: on Unix systems, you can use {\tt ps2pdf} for this
purpose; under Microsoft Windows, you can use Adobe's Distiller, or
if you have cygwin installed, you can use {\tt dvipdf} or
{\tt ps2pdf}.  Note
that some word processing programs generate PDF which may not include
all the necessary fonts (esp. tree diagrams, symbols). When you print
or create the PDF file, there is usually an option in your printer
setup to include none, all or just non-standard fonts.  Please make
sure that you select the option of including ALL the fonts.  {\em
  Before sending it, test your {\/\em PDF} by printing it from a
  computer different from the one where it was created}. Moreover,
some word processor may generate very large postscript/PDF files,
where each page is rendered as an image. Such images may reproduce
poorly.  In this case, try alternative ways to obtain the postscript
and/or PDF.  One way on some systems is to install a driver for a
postscript printer, send your document to the printer specifying
``Output to a file'', then convert the file to PDF.

For reasons of uniformity, Adobe's {\bf Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\begin{quote}
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\end{quote}
in the preamble.

Additionally, it is of utmost importance to specify the {\bf
  US-Letter format} (8.5in $\times$ 11in) when formatting the paper.
When working with {\tt dvips}, for instance, one should specify {\tt
  -t letter}.

Print-outs of the PDF file on US-Letter paper should be identical to the
hardcopy version.  If you cannot meet the above requirements about the
production of your electronic submission, please contact the
publication chairs above  as soon as possible.


\subsection{Layout}
\label{ssec:layout}

Format manuscripts two columns to a page, in the manner these
instructions are formatted. The exact dimensions for a page on US-letter
paper are:

\begin{itemize}
\item Left and right margins: 1in
\item Top margin:1in
\item Bottom margin: 1in
\item Column width: 3.15in
\item Column height: 9in
\item Gap between columns: 0.2in
\end{itemize}

\noindent Papers should not be submitted on any other paper size. Exceptionally,
authors for whom it is \emph{impossible} to format on US-Letter paper,
may format for \emph{A4} paper. In this case, they should keep the \emph{top}
and \emph{left} margins as given above, use the same column width,
height and gap, and modify the bottom and right margins as necessary.
Note that the text will no longer be centered.

\subsection{The First Page}
\label{ssec:first}

Center the title, author's name(s) and affiliation(s) across both
columns. Do not use footnotes for affiliations.  Do not include the
paper ID number assigned during the submission process.
Use the two-column format only when you begin the abstract.

{\bf Title}: Place the title centered at the top of the first page, in
a 15 point bold font.  (For a complete guide to font sizes and styles, see Table~\ref{font-table}.)
Long title should be typed on two lines without
a blank line intervening. Approximately, put the title at 1in from the
top of the page, followed by a blank line, then the author's names(s),
and the affiliation on the following line.  Do not use only initials
for given names (middle initials are allowed). Do not format surnames
in all capitals (e.g., ``Leacock,'' not ``LEACOCK'').  The affiliation should
contain the author's complete address, and if possible an electronic
mail address. Leave about 0.75in between the affiliation and the body
of the first page.

{\bf Abstract}: Type the abstract at the beginning of the first
column.  The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.25in on each side.  Center the word {\bf Abstract} in a 12 point
bold font above the body of the abstract. The abstract should be a
concise summary of the general thesis and conclusions of the paper.
It should be no longer than 200 words.  The abstract text should be in 10 point font.

{\bf Text}: Begin typing the main body of the text immediately after
the abstract, observing the two-column format as shown in
the present document.  Do not include page numbers.

{\bf Indent} when starting a new paragraph. For reasons of uniformity,
use Adobe's {\bf Times Roman} fonts, with 11 points for text and
subsection headings, 12 points for section headings and 15 points for
the title.  If Times Roman is unavailable, use {\bf Computer Modern
  Roman} (\LaTeX2e{}'s default; see section \ref{sect:pdf} above).
Note that the latter is about 10\% less dense than Adobe's Times Roman
font.

\subsection{Sections}

{\bf Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals.

{\bf Citations}: Citations within the text appear
in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
the text itself, as Gusfield~\shortcite{Gusfield:97}.
Append lowercase letters to the year in cases of ambiguities.
Treat double authors as in~\cite{Aho:72}, but write as
in~\cite{Chandra:81} when more than two authors are involved.
Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}.

\textbf{References}: Gather the full set of references together under
the heading {\bf References}; place the section before any Appendices,
unless they contain references. Arrange the references alphabetically
by first author, rather than by order of occurrence in the text.
Provide as complete a citation as possible, using a consistent format,
such as the one for {\em Computational Linguistics\/} or the one in the
{\em Publication Manual of the American
Psychological Association\/}~\cite{APA:83}.  Use of full names for
authors rather than initials is preferred.  A list of abbreviations
for common computer science journals can be found in the ACM
{\em Computing Reviews\/}~\cite{ACM:83}.

The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
American Psychological Association format, allowing regular citations,
short citations and multiple citations as described above.

{\bf Appendices}: Appendices, if any, directly follow the text and the
references (but see above).  Letter them in sequence and provide an
informative title: {\bf Appendix A. Title of Appendix}.

\textbf{Acknowledgment} sections should go as a last (unnumbered) section immediately
before the references.

\subsection{Footnotes}

{\bf Footnotes}: Put footnotes at the bottom of the page. They may
be numbered or referred to by asterisks or other
symbols.\footnote{This is how a footnote should appear.} Footnotes
should be separated from the text by a line.\footnote{Note the
line separating the footnotes from the text.}  Footnotes should be in 9 point font.

\subsection{Graphics}

{\bf Illustrations}: Place figures, tables, and photographs in the
paper near where they are first discussed, rather than at the end, if
possible.  Wide illustrations may run across both columns and should be placed at
the top of a page. Color illustrations are discouraged, unless you have verified that
they will be understandable when printed in black ink.

\begin{table}
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
abstract text & 10 pt & \\
captions & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

{\bf Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and
tables below the body, using 10 point text.

\section{Length of Submission}
\label{sec:length}

The NAACL HLT 2010 main conference accepts submissions of long papers
and short papers.  The maximum length of a long paper manuscript is
eight (8) pages of content and one (1) additional page of references
\emph{only} (appendices count against the eight pages, not the
additional one page).  The maximum length of a short paper manuscript
is four (4) pages including references.  For both long and short
papers, all illustrations, references, and appendices must be
accommodated within these page limits, observing the formatting
instructions given in the present document.  Papers that do not
conform to the specified length and formatting requirements are
subject to be rejected without review.

% Up to two (2) additional pages may be purchased from ACL at the
% price of \$250 per page; please contact the publication chairs above
% for more information about this option.

\section*{Acknowledgments}

Do not number the acknowledgment section.

\begin{thebibliography}{}

\bibitem[\protect\citename{Specht}1991]{Specht:91}
Donald F. Specht.
\newblock 1991.
\newblock A General Regression Neural Network.
\newblock {\em Neural Networks},
2(6):568--576.

\bibitem[\protect\citename{Gers \bgroup et al.\egroup }1999]{Gers:99}
Felix A. Gers,  Jurgen Schmidhuber, and Fred Cummins.
\newblock 1999.
\newblock Learning to Forget: Continual Prediction With LSTM.
\newblock {\em Artificial Neural Networks},
2(470):850--855.

\bibitem[\protect\citename{Gers and Schmidhuber}2001]{Gers:01}
Felix A. Gers and Jurgen Schmidhuber.
\newblock 2001.
\newblock LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages.
\newblock {\em IEEE Transactions on Neural Networks},
12(6):1333-1340.

\bibitem[\protect\citename{Gers \bgroup et al.\egroup }2002]{Gers:02}
Felix A. Gers, Nicol N. Schraudolph, and Jurgen Schmidhuber.
\newblock 2002.
\newblock Learning Precise Timing with LSTM Recurrent Networks.
\newblock {\em Journal of Machine Learning Research},
3:115--143.

\bibitem[\protect\citename{Vinyals \bgroup et al.\egroup }2014]{Vinyals:14}
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
\newblock 2014.
\newblock Show and Tell: A Neural Image Caption Generator.
\newblock {\em CoRR},
abs/1411.4555.

\bibitem[\protect\citename{Hochreiter and Schmidhuber}1997]{Hochreiter:97}
Sepp Hochreiter and Jurgen Schmidhuber.
\newblock 1997.
\newblock Long Short-Term Memory.
\newblock {\em Neural Computation},
9(8):1735--1780.

\end{thebibliography}

\end{document}
