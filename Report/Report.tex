%
% File naaclhlt2010.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{CS 475 Intro to Machine Learning: Long Short-Term Memory Project Report}

\author{Charlie Wang\\
  Johns Hopkins University\\
  3400 N. Charles Street\\
  Baltimore, MD 21218, USA\\
  {\tt TODO: email@email.com}
  \And
  Gilbert Maystre \\
  Johns Hopkins University\\
  3400 N. Charles Street\\
  Baltimore, MD 21218, USA\\
  {\tt TODO: email@email.com}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  TODO: This document contains the instructions for preparing a camera-ready
  manuscript for the proceedings of NAACL HLT 2010. The document itself conforms
  to its own specifications, and is therefore an example of what
  your manuscript should look like.  Authors are asked to conform to
  all the directions reported in this document.
\end{abstract}

\section{Introduction}

Long Short-Term Memory (LSTM) is an artificial neural network architecture, which is in turn considered as a machine learning model. First proposed by ~\cite{Hochreiter:97}, LSTMs are a subclass of recurrent neural networks, a subclass of artificial neural networks. LSTMs build upon the decades of artificial neural network research to solve new types of problems.

The artificial neural network architecture is based on the idea of biological neurons in the brain and how neurons are organized. It was discovered that neurons receive multiple inputs from other neurons, does something simple with the inputs, and outputs a signal which can be relayed to many other neurons. In the same idea, the nodes in artificial neural networks replicate the biological neurons. Artificial neural network nodes receive multiple inputs from other nodes, which are weighted. A summation is performed on the inputs, and this sum is then passed through an activation function. Many times, the activation function is a simple logistic sigmoid function. Then, the node would output the value to other nodes through weighted connections. Although each neuron is seemingly simple of-by-themselves, together, they can solve many complex problems, including the estimation of a regression surface by ~\cite{Specht:91}. The artificial neural networks were generally organized with an input layer, which consisted of many input nodes with an identity activation function. The nodes in the input layer would be connected to the nodes in the hidden layer, where the nodes in the hidden layer would in turn be connected to the nodes in the output layer. This high-level overview is the general architecture of artificial neural networks.

Much work was done on artificial neural networks, such as adding many hidden layers to form deep learning neural networks. Another work that arose in the artificial neural network world was with recurrent neural networks. Recurrent neural networks are similar to a traditional artificial neural network architecture of input layer, hidden layer, and output layer. However, the main difference is that the weights coming out of a node does not just go to other nodes in other layers -- the weights can loop back to the node itself or other nodes in that respective layer. This new idea became the recurrent neural network.

Recurrent neural networks were able to extend upon artificial neural networks, but more was wanted from what recurrent neural networks were able to perform and achieve. This led to the birth of architecture of Long Short-Term Memory (LSTM), which is built on recurrent neural networks, which is in turn built on artificial neural networks in general. Based on the work of ~\cite{Hochreiter:97}, greater complexity was added to the nodes in the hidden layer of the neural network. To start, the idea of a hidden layer node was extended into the idea of a memory block. For each memory block, it consists of the node itself, which is renamed as a cell, and the novel idea of gates. Each memory block had two gates, an input gate and an output gate. The gates were considered multiplicative, and the output of the gates are multiplied with the respective weighted values coming from and to the cell in the memory block. To clarify, the output of the input gate is multiplied with the summed input weights to the memory block. The output of the output gate is multiplied with the output value of the memory block. The goal of the gates is to control the constant error flow through discrete time steps due to the effect of the loops in the LSTM. Thus, the need for an activation function for the cell was discarded, and instead, an edge that flows back into the input of the cell was added. The value of the cell was named cell state.

Recall that the LSTM proposed by ~\cite{Hochreiter:97} is built on the idea of recurrent neural network. Thus, the general overview of the connections between nodes can be illustrated in this way: each input node output is connected/flows to each of the memory block, each gate, and each output node. The output of the memory blocks is connected to the memory block itself (the recurrent part), the output nodes, and the gates.

More research was conducted on the LSTM architecture, and ~\cite{Gers:99} discovered that there was a flaw with the cell. In fact, due to the loop from the output of the cell to the input of the cell on each time step, ~\cite{Gers:99} discovered that the loop could cause the cell state to grow indefinitely, thus causing the whole network to be skewed so much that the output of the network is of not much use. In other words, the whole network becomes broken after too many time steps. Thus, ~\cite{Gers:99} added onto the complexity of the original LSTM architecture by adding an additional gate, which they named the forget gate. The forget gate is similar to the input and output gates, but instead of targeting the input to the cell like the input gate or the output of the cell like the output gate, the forget gate targeted that recurrent loop of the cell. Similarly, as with the input gate and the output gate, they received weighted inputs from the input nodes and the memory block itself. Thus, the number of weights in the network grew and the complexity of the memory block grew, but ~\cite{Gers:99} were able to show that this improved the network results in the long term.

However, there was still a flaw present in the LSTM architecture. The LSTM architecture after the work of ~\cite{Gers:99} was ill-suited for tasks where accurate measurement of time intervals was needed. For example, the distinction is needed in some tasks to know when spikes or other abnormalities occur either 100 times steps apart, or 101 time steps apart. Thus, ~\cite{Gers:02} proposed the idea of adding peephole weighted connection from the cell to the input, forget, and output gates. Without the peephole connections, the gates would only be able to see what the output of the cell is after being multiplied by the output of the output gate. If the output gate is closed, in other words close to 0, the gates would not be receiving an accurate picture of what the cell state is giving as an output. Thus, with the peephole connection which serve as a direction connection for gates to see what the cell state is currently like, network performance was shown to increase by ~\cite{Gers:02}.

These advancements, including many other following advancements, have made the LSTM architecture applicable to many problems out in the world today. For example, Google was able to achieve breakthrough in generating image captions with the help of the LSTM architecture combined with a deep convolutional net, which is described in ~\cite{Vinyals:14}. In fact, in this specific example, natural language captions were able to be generated on images by recognizing objects in the picture, such as what was the main subject and what the background is like.

The capabilities, complexities, and potential of LSTM is why the project was chosen to be done on the LSTM architecture and on the application of a LSTM network. For the project, an application of LSTM was chosen that incorporates some of the important advancements with the architecture, such as by ~\cite{Gers:99} and by ~\cite{Gers:02}. Thus, the project is conducted on the works of ~\cite{Gers:01}, which is on the topic of learning simple context-sensitive languages (CSL).

\section{Problem}
Problem: talk about what the problem is and how it is set up. What language (CSL) is generated ($a^n b^n$). How the network is suppose to work. How prediction works. How the network knows is right or wrong. Say that using supervised learning. Ultimate goal is to classify what. Why need to use LSTM, cannot use recurrent neural networks.

\section{Network Architecture}
Network Architecture: How is the architecture set up. How many input nodes, hidden nodes, output nodes. How many layers. How nodes are connected. What about the weights? What is expected input and expected output from the network.

\section{Algorithm}
Algorithm: Talk about the algorithms that are used. Analyze the algorithm: how does it work, why it works, etc. Is gradient learning, etc.

\section{Coding the Solution}
Coding: how approached the problem. How prepare data? first generate language. Then get understanding of paper and how network is set up, which is highlighted in the section before under architecture. !!!Problems encountered.!!! Then coding the forward pass, derivatives, and the backward pass. How there is little information on the backward pass. Needed to go and find it. Talk about how much time spent on the project and how much work was done/

Also, analyze the algorithm using machine learning concepts/understandings.

\section{Results}
Results: analyze the results. Explain why get results.

\section{Comparison to Proposal}
Comparison to Proposal:
What was different. If different, give good explanation.

\section{Conclusion}




\section{Template: General Instructions}

Manuscripts must be in two-column format.  Exceptions to the
two-column format include the title, authors' names and complete
addresses, which must be centered at the top of the first page (see
the guidelines in Subsection~\ref{ssec:first}), and any full-width
figures or tables .  Type single-spaced.  Do not number the pages.
Start all pages directly under the top margin.  See the guidelines
later regarding formatting the first page.

%% If the paper is produced by a printer, make sure that the quality
%% of the output is dark enough to photocopy well.  It may be necessary
%% to have your laser printer adjusted for this purpose.  Papers that are too
%% faint to reproduce well may not be included.

%% {\bf Do not print page numbers on the manuscript.}  Write them lightly
%% on the back of each page in the upper left corner along with the
%% (first) author's name.

The maximum length of a manuscript is eight (8) pages for the main
conference, printed single-sided, plus one (1) page for references
(see Section~\ref{sec:length} for additional information on the
maximum number of pages).  Do not number the pages.

\subsection{Electronically-available resources}

NAACL HLT provides this description in \LaTeX2e{} ({\tt naaclhlt2010.tex}) and PDF
format ({\tt naaclhlt2010.pdf}), along with the \LaTeX2e{} style file used to
format it ({\tt naaclhlt2010.sty}) and an ACL bibliography style ({\tt naaclhlt2010.bst}).
These files are all available at
{\tt http://naaclhlt2010.isi.edu}.  A Microsoft Word
template file ({\tt naaclhlt2010.dot}) is also available at the same URL. We
strongly recommend the use of these style files, which have been
appropriately tailored for the NAACL HLT 2010 proceedings.


\subsection{Format of Electronic Manuscript}
\label{sect:pdf}

For the production of the electronic manuscript you must use Adobe's
Portable Document Format (PDF). This format can be generated from
postscript files: on Unix systems, you can use {\tt ps2pdf} for this
purpose; under Microsoft Windows, you can use Adobe's Distiller, or
if you have cygwin installed, you can use {\tt dvipdf} or
{\tt ps2pdf}.  Note
that some word processing programs generate PDF which may not include
all the necessary fonts (esp. tree diagrams, symbols). When you print
or create the PDF file, there is usually an option in your printer
setup to include none, all or just non-standard fonts.  Please make
sure that you select the option of including ALL the fonts.  {\em
  Before sending it, test your {\/\em PDF} by printing it from a
  computer different from the one where it was created}. Moreover,
some word processor may generate very large postscript/PDF files,
where each page is rendered as an image. Such images may reproduce
poorly.  In this case, try alternative ways to obtain the postscript
and/or PDF.  One way on some systems is to install a driver for a
postscript printer, send your document to the printer specifying
``Output to a file'', then convert the file to PDF.

For reasons of uniformity, Adobe's {\bf Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\begin{quote}
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\end{quote}
in the preamble.

Additionally, it is of utmost importance to specify the {\bf
  US-Letter format} (8.5in $\times$ 11in) when formatting the paper.
When working with {\tt dvips}, for instance, one should specify {\tt
  -t letter}.

Print-outs of the PDF file on US-Letter paper should be identical to the
hardcopy version.  If you cannot meet the above requirements about the
production of your electronic submission, please contact the
publication chairs above  as soon as possible.


\subsection{Layout}
\label{ssec:layout}

Format manuscripts two columns to a page, in the manner these
instructions are formatted. The exact dimensions for a page on US-letter
paper are:

\begin{itemize}
\item Left and right margins: 1in
\item Top margin:1in
\item Bottom margin: 1in
\item Column width: 3.15in
\item Column height: 9in
\item Gap between columns: 0.2in
\end{itemize}

\noindent Papers should not be submitted on any other paper size. Exceptionally,
authors for whom it is \emph{impossible} to format on US-Letter paper,
may format for \emph{A4} paper. In this case, they should keep the \emph{top}
and \emph{left} margins as given above, use the same column width,
height and gap, and modify the bottom and right margins as necessary.
Note that the text will no longer be centered.

\subsection{The First Page}
\label{ssec:first}

Center the title, author's name(s) and affiliation(s) across both
columns. Do not use footnotes for affiliations.  Do not include the
paper ID number assigned during the submission process.
Use the two-column format only when you begin the abstract.

{\bf Title}: Place the title centered at the top of the first page, in
a 15 point bold font.  (For a complete guide to font sizes and styles, see Table~\ref{font-table}.)
Long title should be typed on two lines without
a blank line intervening. Approximately, put the title at 1in from the
top of the page, followed by a blank line, then the author's names(s),
and the affiliation on the following line.  Do not use only initials
for given names (middle initials are allowed). Do not format surnames
in all capitals (e.g., ``Leacock,'' not ``LEACOCK'').  The affiliation should
contain the author's complete address, and if possible an electronic
mail address. Leave about 0.75in between the affiliation and the body
of the first page.

{\bf Abstract}: Type the abstract at the beginning of the first
column.  The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.25in on each side.  Center the word {\bf Abstract} in a 12 point
bold font above the body of the abstract. The abstract should be a
concise summary of the general thesis and conclusions of the paper.
It should be no longer than 200 words.  The abstract text should be in 10 point font.

{\bf Text}: Begin typing the main body of the text immediately after
the abstract, observing the two-column format as shown in
the present document.  Do not include page numbers.

{\bf Indent} when starting a new paragraph. For reasons of uniformity,
use Adobe's {\bf Times Roman} fonts, with 11 points for text and
subsection headings, 12 points for section headings and 15 points for
the title.  If Times Roman is unavailable, use {\bf Computer Modern
  Roman} (\LaTeX2e{}'s default; see section \ref{sect:pdf} above).
Note that the latter is about 10\% less dense than Adobe's Times Roman
font.

\subsection{Sections}

{\bf Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals.

{\bf Citations}: Citations within the text appear
in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
the text itself, as Gusfield~\shortcite{Gusfield:97}.
Append lowercase letters to the year in cases of ambiguities.
Treat double authors as in~\cite{Aho:72}, but write as
in~\cite{Chandra:81} when more than two authors are involved.
Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}.

\textbf{References}: Gather the full set of references together under
the heading {\bf References}; place the section before any Appendices,
unless they contain references. Arrange the references alphabetically
by first author, rather than by order of occurrence in the text.
Provide as complete a citation as possible, using a consistent format,
such as the one for {\em Computational Linguistics\/} or the one in the
{\em Publication Manual of the American
Psychological Association\/}~\cite{APA:83}.  Use of full names for
authors rather than initials is preferred.  A list of abbreviations
for common computer science journals can be found in the ACM
{\em Computing Reviews\/}~\cite{ACM:83}.

The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
American Psychological Association format, allowing regular citations,
short citations and multiple citations as described above.

{\bf Appendices}: Appendices, if any, directly follow the text and the
references (but see above).  Letter them in sequence and provide an
informative title: {\bf Appendix A. Title of Appendix}.

\textbf{Acknowledgment} sections should go as a last (unnumbered) section immediately
before the references.

\subsection{Footnotes}

{\bf Footnotes}: Put footnotes at the bottom of the page. They may
be numbered or referred to by asterisks or other
symbols.\footnote{This is how a footnote should appear.} Footnotes
should be separated from the text by a line.\footnote{Note the
line separating the footnotes from the text.}  Footnotes should be in 9 point font.

\subsection{Graphics}

{\bf Illustrations}: Place figures, tables, and photographs in the
paper near where they are first discussed, rather than at the end, if
possible.  Wide illustrations may run across both columns and should be placed at
the top of a page. Color illustrations are discouraged, unless you have verified that
they will be understandable when printed in black ink.

\begin{table}
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
abstract text & 10 pt & \\
captions & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

{\bf Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and
tables below the body, using 10 point text.

\section{Length of Submission}
\label{sec:length}

The NAACL HLT 2010 main conference accepts submissions of long papers
and short papers.  The maximum length of a long paper manuscript is
eight (8) pages of content and one (1) additional page of references
\emph{only} (appendices count against the eight pages, not the
additional one page).  The maximum length of a short paper manuscript
is four (4) pages including references.  For both long and short
papers, all illustrations, references, and appendices must be
accommodated within these page limits, observing the formatting
instructions given in the present document.  Papers that do not
conform to the specified length and formatting requirements are
subject to be rejected without review.

% Up to two (2) additional pages may be purchased from ACL at the
% price of \$250 per page; please contact the publication chairs above
% for more information about this option.

\section*{Acknowledgments}

Do not number the acknowledgment section.

\begin{thebibliography}{}

\bibitem[\protect\citename{Specht}1991]{Specht:91}
Donald F. Specht.
\newblock 1991.
\newblock A General Regression Neural Network.
\newblock {\em Neural Networks},
2(6):568--576.

\bibitem[\protect\citename{Gers \bgroup et al.\egroup }1999]{Gers:99}
Felix A. Gers,  Jurgen Schmidhuber, and Fred Cummins.
\newblock 1999.
\newblock Learning to Forget: Continual Prediction With LSTM.
\newblock {\em Artificial Neural Networks},
2(470):850--855.

\bibitem[\protect\citename{Gers and Schmidhuber}2001]{Gers:01}
Felix A. Gers and Jurgen Schmidhuber.
\newblock 2001.
\newblock LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages.
\newblock {\em IEEE Transactions on Neural Networks},
12(6):1333-1340.

\bibitem[\protect\citename{Gers \bgroup et al.\egroup }2002]{Gers:02}
Felix A. Gers, Nicol N. Schraudolph, and Jurgen Schmidhuber.
\newblock 2002.
\newblock Learning Precise Timing with LSTM Recurrent Networks.
\newblock {\em Journal of Machine Learning Research},
3:115--143.

\bibitem[\protect\citename{Vinyals \bgroup et al.\egroup }2014]{Vinyals:14}
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
\newblock 2014.
\newblock Show and Tell: A Neural Image Caption Generator.
\newblock {\em CoRR},
abs/1411.4555.

\bibitem[\protect\citename{Hochreiter and Schmidhuber}1997]{Hochreiter:97}
Sepp Hochreiter and Jurgen Schmidhuber.
\newblock 1997.
\newblock Long Short-Term Memory.
\newblock {\em Neural Computation},
9(8):1735--1780.

\end{thebibliography}

\end{document}
